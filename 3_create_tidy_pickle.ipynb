{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ddef73d-6d08-4b59-961d-66ce683c559e",
   "metadata": {},
   "source": [
    "# Create clean files \n",
    "## with all the dates and calculations done to import later \n",
    "\n",
    "env: running with **raster** right now since I do not need rioxarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50ca8d7f-188c-4a67-8310-042df0f5337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages \n",
    "# %matplotlib widget\n",
    "%matplotlib inline\n",
    "\n",
    "# plotting packages \n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "# data packages \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from datetime import datetime\n",
    "\n",
    "import csv \n",
    "import copy \n",
    "import os.path \n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea398108-f323-44f2-9916-95a5edfc72a2",
   "metadata": {},
   "source": [
    "Open CSVs using code from John's Github, starting here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f33e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv file for Site Openness and Snow Disappearance\n",
    "SDD23 = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\Data\\\\field_data_proccessed_byJohn\\\\FinalData\\\\GapFraction_SDD.csv\")\n",
    "# SDD23 = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\other\\\\Data\\\\field_data_proccessed_byJohn\\\\FinalData\\\\GapFraction_SDD.csv\")\n",
    "\n",
    "# I went in to fill the missing gap fraction from John's code with Susan's (from her data) for the forest and gap sites \n",
    "# These changes are in the \"edit\" file \n",
    "# SDD23 = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\other\\\\Data\\\\field_data_proccessed_byJohn\\\\FinalData\\\\GapFraction_SDD_edit.csv\")\n",
    "\n",
    "\n",
    "#make sure all dates are type datetime\n",
    "SDD23['SDD1'] = pd.to_datetime(SDD23['SDD1'])\n",
    "SDD23['SDD2'] = pd.to_datetime(SDD23['SDD2'])\n",
    "SDD23['SDD3'] = pd.to_datetime(SDD23['SDD3'])\n",
    "SDD23['SDD4'] = pd.to_datetime(SDD23['SDD4'])\n",
    "SDD23['SDD5'] = pd.to_datetime(SDD23['SDD5'])\n",
    "SDD23['SDD6'] = pd.to_datetime(SDD23['SDD6'])\n",
    "SDD23['SDD7'] = pd.to_datetime(SDD23['SDD7'])\n",
    "SDD23['SDD8'] = pd.to_datetime(SDD23['SDD8'])\n",
    "SDD23['SDD9'] = pd.to_datetime(SDD23['SDD9'])\n",
    "\n",
    "#create variables containing SDD23 for north and south respectively\n",
    "SDD_CN23 = SDD23.loc[SDD23['Site'] == \"CN\"]\n",
    "SDD_CS23 = SDD23.loc[SDD23['Site'] == \"CS\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c464f64",
   "metadata": {},
   "source": [
    "Now, snow depth csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a07539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sites that match susan's sites\n",
    "CNF23 = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\Data\\\\field_data_proccessed_byJohn\\\\FinalData\\\\ProcessedCameraData\\\\CN-FProcessedData.csv\")\n",
    "CNG23 = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\Data\\\\field_data_proccessed_byJohn\\\\FinalData\\\\ProcessedCameraData\\\\CN-GProcessedData.csv\")\n",
    "\n",
    "CSF23  = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\Data\\\\field_data_proccessed_byJohn\\\\FinalData\\\\ProcessedCameraData\\\\CS-FProcessedData.csv\")\n",
    "CSG23  = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\Data\\\\field_data_proccessed_byJohn\\\\FinalData\\\\ProcessedCameraData\\\\CS-GProcessedData.csv\")\n",
    "\n",
    "# the other sites with forest perscriptions\n",
    "CN20  = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\Data\\\\field_data_proccessed_byJohn\\\\FinalData\\\\ProcessedCameraData\\\\CN-20ProcessedData.csv\")\n",
    "CN50  = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\Data\\\\field_data_proccessed_byJohn\\\\FinalData\\\\ProcessedCameraData\\\\CN-50ProcessedData.csv\")\n",
    "CN60  = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\Data\\\\field_data_proccessed_byJohn\\\\FinalData\\\\ProcessedCameraData\\\\CN-60ProcessedData.csv\")\n",
    "CN70  = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\Data\\\\field_data_proccessed_byJohn\\\\FinalData\\\\ProcessedCameraData\\\\CN-70ProcessedData.csv\")\n",
    "\n",
    "CS20   = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\Data\\\\field_data_proccessed_byJohn\\\\FinalData\\\\ProcessedCameraData\\\\CS-20ProcessedData.csv\")\n",
    "CS50   = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\Data\\\\field_data_proccessed_byJohn\\\\FinalData\\\\ProcessedCameraData\\\\CS-50ProcessedData.csv\")\n",
    "CS60   = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\Data\\\\field_data_proccessed_byJohn\\\\FinalData\\\\ProcessedCameraData\\\\CS-60ProcessedData.csv\")\n",
    "CS120  = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\Data\\\\field_data_proccessed_byJohn\\\\FinalData\\\\ProcessedCameraData\\\\CS-120ProcessedData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347230ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, make date column datetime object\n",
    "CNG23['Date'] = pd.to_datetime(CNG23['Date'])\n",
    "CNF23['Date'] = pd.to_datetime(CNF23['Date'])\n",
    "CSG23['Date'] = pd.to_datetime(CSG23['Date'])\n",
    "CSF23['Date'] = pd.to_datetime(CSF23['Date'])\n",
    "\n",
    "CN20['Date'] = pd.to_datetime(CN20['Date'])\n",
    "CN50['Date'] = pd.to_datetime(CN50['Date'])\n",
    "CN60['Date'] = pd.to_datetime(CN60['Date'])\n",
    "CN70['Date'] = pd.to_datetime(CN70['Date'])\n",
    "\n",
    "CS20['Date'] = pd.to_datetime(CS20['Date'])\n",
    "CS50['Date'] = pd.to_datetime(CS50['Date'])\n",
    "CS60['Date'] = pd.to_datetime(CS60['Date'])\n",
    "CS120['Date'] = pd.to_datetime(CS120['Date'])\n",
    "\n",
    "# set date column as index\n",
    "CNF23.index = pd.DatetimeIndex(CNF23['Date'])\n",
    "CNG23.index = pd.DatetimeIndex(CNG23['Date'])\n",
    "CSF23.index = pd.DatetimeIndex(CSF23['Date'])\n",
    "CSG23.index = pd.DatetimeIndex(CSG23['Date'])\n",
    "\n",
    "CN20.index = pd.DatetimeIndex(CN20['Date'])\n",
    "CN50.index = pd.DatetimeIndex(CN50['Date'])\n",
    "CN60.index = pd.DatetimeIndex(CN60['Date'])\n",
    "CN70.index = pd.DatetimeIndex(CN70['Date'])\n",
    "\n",
    "CS20.index = pd.DatetimeIndex(CS20['Date'])\n",
    "CS50.index = pd.DatetimeIndex(CS50['Date'])\n",
    "CS60.index = pd.DatetimeIndex(CS60['Date'])\n",
    "CS120.index = pd.DatetimeIndex(CS120['Date'])\n",
    "\n",
    "\n",
    "# Not doing this... \n",
    "# # drop extra date column\n",
    "# CNF23.drop(columns=['Date'], inplace=True)\n",
    "# CNG23.drop(columns=['Date'], inplace=True)\n",
    "# CSF23.drop(columns=['Date'], inplace=True)\n",
    "# CSG23.drop(columns=['Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfedda5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate statistics for snow depth (minimum depth, maximum depth, and median depth)\n",
    "#Because there are only three poles for each site, each statistic represents the value of a single pole\n",
    "# north sites\n",
    "CNF23['Median_depth'] = CNF23[[\"Pole1\",\"Pole2\",\"Pole3\"]].median(axis=1)\n",
    "CNF23['Maximum_depth'] = CNF23[[\"Pole1\",\"Pole2\",\"Pole3\"]].max(axis=1)\n",
    "CNF23['Minimum_depth'] = CNF23[[\"Pole1\",\"Pole2\",\"Pole3\"]].min(axis=1)\n",
    "CNG23['Median_depth'] = CNG23[[\"Pole1\",\"Pole2\",\"Pole3\"]].median(axis=1)\n",
    "CNG23['Maximum_depth'] = CNG23[[\"Pole1\",\"Pole2\",\"Pole3\"]].max(axis=1)\n",
    "CNG23['Minimum_depth'] = CNG23[[\"Pole1\",\"Pole2\",\"Pole3\"]].min(axis=1)\n",
    "\n",
    "CN20['Median_depth'] = CN20[[\"Pole1\",\"Pole2\",\"Pole3\"]].median(axis=1)\n",
    "CN20['Maximum_depth'] = CN20[[\"Pole1\",\"Pole2\",\"Pole3\"]].max(axis=1)\n",
    "CN20['Minimum_depth'] = CN20[[\"Pole1\",\"Pole2\",\"Pole3\"]].min(axis=1)\n",
    "CN50['Median_depth'] = CN50[[\"Pole1\",\"Pole2\",\"Pole3\"]].median(axis=1)\n",
    "CN50['Maximum_depth'] = CN50[[\"Pole1\",\"Pole2\",\"Pole3\"]].max(axis=1)\n",
    "CN50['Minimum_depth'] = CN50[[\"Pole1\",\"Pole2\",\"Pole3\"]].min(axis=1)\n",
    "CN60['Median_depth'] = CN60[[\"Pole1\",\"Pole2\",\"Pole3\"]].median(axis=1)\n",
    "CN60['Maximum_depth'] = CN60[[\"Pole1\",\"Pole2\",\"Pole3\"]].max(axis=1)\n",
    "CN60['Minimum_depth'] = CN60[[\"Pole1\",\"Pole2\",\"Pole3\"]].min(axis=1)\n",
    "CN70['Median_depth'] = CN70[[\"Pole1\",\"Pole2\",\"Pole3\"]].median(axis=1)\n",
    "CN70['Maximum_depth'] = CN70[[\"Pole1\",\"Pole2\",\"Pole3\"]].max(axis=1)\n",
    "CN70['Minimum_depth'] = CN70[[\"Pole1\",\"Pole2\",\"Pole3\"]].min(axis=1)\n",
    "\n",
    "# south sites\n",
    "CSF23['Median_depth'] = CSF23[[\"Pole1\",\"Pole2\",\"Pole3\"]].median(axis=1)\n",
    "CSF23['Maximum_depth'] = CSF23[[\"Pole1\",\"Pole2\",\"Pole3\"]].max(axis=1)\n",
    "CSF23['Minimum_depth'] = CSF23[[\"Pole1\",\"Pole2\",\"Pole3\"]].min(axis=1)\n",
    "CSG23['Median_depth'] = CSG23[[\"Pole1\",\"Pole2\",\"Pole3\"]].median(axis=1)\n",
    "CSG23['Maximum_depth'] = CSG23[[\"Pole1\",\"Pole2\",\"Pole3\"]].max(axis=1)\n",
    "CSG23['Minimum_depth'] = CSG23[[\"Pole1\",\"Pole2\",\"Pole3\"]].min(axis=1)\n",
    "CS20['Median_depth'] = CS20[[\"Pole1\",\"Pole2\",\"Pole3\"]].median(axis=1)\n",
    "\n",
    "CS20['Maximum_depth'] = CS20[[\"Pole1\",\"Pole2\",\"Pole3\"]].max(axis=1)\n",
    "CS20['Minimum_depth'] = CS20[[\"Pole1\",\"Pole2\",\"Pole3\"]].min(axis=1)\n",
    "CS50['Median_depth'] = CS50[[\"Pole1\",\"Pole2\",\"Pole3\"]].median(axis=1)\n",
    "CS50['Maximum_depth'] = CS50[[\"Pole1\",\"Pole2\",\"Pole3\"]].max(axis=1)\n",
    "CS50['Minimum_depth'] = CS50[[\"Pole1\",\"Pole2\",\"Pole3\"]].min(axis=1)\n",
    "CS60['Median_depth'] = CS60[[\"Pole1\",\"Pole2\",\"Pole3\"]].median(axis=1)\n",
    "CS60['Maximum_depth'] = CS60[[\"Pole1\",\"Pole2\",\"Pole3\"]].max(axis=1)\n",
    "CS60['Minimum_depth'] = CS60[[\"Pole1\",\"Pole2\",\"Pole3\"]].min(axis=1)\n",
    "CS120['Median_depth'] = CS120[[\"Pole1\",\"Pole2\",\"Pole3\"]].median(axis=1)\n",
    "CS120['Maximum_depth'] = CS120[[\"Pole1\",\"Pole2\",\"Pole3\"]].max(axis=1)\n",
    "CS120['Minimum_depth'] = CS120[[\"Pole1\",\"Pole2\",\"Pole3\"]].min(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295e5a41",
   "metadata": {},
   "source": [
    "And, Susan's data from 2021 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349a9199",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data from WY21\n",
    "CNF21 = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\Data\\\\field_data_Dickersonetal2023\\\\SnowDepthFromTimelapsePhotos\\\\CNF_WDNR-M6_WY21.csv\", skiprows=[0])\n",
    "CNG21 = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\Data\\\\field_data_Dickersonetal2023\\\\SnowDepthFromTimelapsePhotos\\\\CNG_WDNR-M1_WY21.csv\", skiprows=[0])\n",
    "\n",
    "CSF21 = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\Data\\\\field_data_Dickersonetal2023\\\\SnowDepthFromTimelapsePhotos\\\\CSF_WDNR-A4_WY21.csv\", skiprows=[0])\n",
    "CSG21 = pd.read_csv(\"E:\\\\CassieLumbrazo\\\\Data\\\\field_data_Dickersonetal2023\\\\SnowDepthFromTimelapsePhotos\\\\CSG_WDNR-M4_WY21.csv\", skiprows=[0])\n",
    "\n",
    "# make date column datetime object\n",
    "CNF21['Date'] = pd.to_datetime(CNF21['Date'])\n",
    "CNG21['Date'] = pd.to_datetime(CNG21['Date'])\n",
    "CSF21['Date'] = pd.to_datetime(CSF21['Date'])\n",
    "CSG21['Date'] = pd.to_datetime(CSG21['Date'])\n",
    "\n",
    "# set date column as index\n",
    "CNF21.index = pd.DatetimeIndex(CNF21['Date'])\n",
    "CNG21.index = pd.DatetimeIndex(CNG21['Date'])\n",
    "CSF21.index = pd.DatetimeIndex(CSF21['Date'])\n",
    "CSG21.index = pd.DatetimeIndex(CSG21['Date'])\n",
    "\n",
    "# # drop extra date column\n",
    "# CNF21.drop(columns=['Date'], inplace=True)\n",
    "# CNG21.drop(columns=['Date'], inplace=True)\n",
    "# CSF21.drop(columns=['Date'], inplace=True)\n",
    "# CSG21.drop(columns=['Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abe0225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WY2021\n",
    "CNF21['Median_depth']  = CNF21[[\"PoleL\",\"PoleC\",\"PoleR\"]].median(axis=1)\n",
    "CNF21['Maximum_depth'] = CNF21[[\"PoleL\",\"PoleC\",\"PoleR\"]].max(axis=1)\n",
    "CNF21['Minimum_depth'] = CNF21[[\"PoleL\",\"PoleC\",\"PoleR\"]].min(axis=1)\n",
    "\n",
    "# there is no center pole for this site \n",
    "CNG21['Median_depth']  = CNG21[[\"PoleL\",\"PoleR\"]].median(axis=1) \n",
    "CNG21['Maximum_depth'] = CNG21[[\"PoleL\",\"PoleR\"]].max(axis=1)\n",
    "CNG21['Minimum_depth'] = CNG21[[\"PoleL\",\"PoleR\"]].min(axis=1)\n",
    "\n",
    "CSF21['Median_depth']  = CSF21[[\"PoleL\",\"PoleC\",\"PoleR\"]].median(axis=1)\n",
    "CSF21['Maximum_depth'] = CSF21[[\"PoleL\",\"PoleC\",\"PoleR\"]].max(axis=1)\n",
    "CSF21['Minimum_depth'] = CSF21[[\"PoleL\",\"PoleC\",\"PoleR\"]].min(axis=1)\n",
    "CSG21['Median_depth']  = CSG21[[\"PoleL\",\"PoleC\",\"PoleR\"]].median(axis=1)\n",
    "CSG21['Maximum_depth'] = CSG21[[\"PoleL\",\"PoleC\",\"PoleR\"]].max(axis=1)\n",
    "CSG21['Minimum_depth'] = CSG21[[\"PoleL\",\"PoleC\",\"PoleR\"]].min(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63e81a-94fb-47a4-9c4f-1b83f4ef1337",
   "metadata": {},
   "source": [
    "### Write the cleaned dataset to pickle for later, or to new csvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12252be1-92e6-45a5-bc58-22a0688e504f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # forest and gap sites \n",
    "# pickle.dump(CNF21, open(\"C:\\\\Users\\\\Lumbr\\\\OneDrive - UW\\\\Documents\\\\Washington\\\\EasternCascades\\\\Python\\\\CER_Timeseries_Analysis\\\\Pickle\\\\CNF21.pkl\", \"wb\"))\n",
    "# pickle.dump(CNG21, open(\"C:\\\\Users\\\\Lumbr\\\\OneDrive - UW\\\\Documents\\\\Washington\\\\EasternCascades\\\\Python\\\\CER_Timeseries_Analysis\\\\Pickle\\\\CNG21.pkl\", \"wb\"))\n",
    "# pickle.dump(CSF21, open(\"C:\\\\Users\\\\Lumbr\\\\OneDrive - UW\\\\Documents\\\\Washington\\\\EasternCascades\\\\Python\\\\CER_Timeseries_Analysis\\\\Pickle\\\\CSF21.pkl\", \"wb\"))\n",
    "# pickle.dump(CSG21, open(\"C:\\\\Users\\\\Lumbr\\\\OneDrive - UW\\\\Documents\\\\Washington\\\\EasternCascades\\\\Python\\\\CER_Timeseries_Analysis\\\\Pickle\\\\CSG21.pkl\", \"wb\"))\n",
    "\n",
    "# pickle.dump(CNF23, open(\"C:\\\\Users\\\\Lumbr\\\\OneDrive - UW\\\\Documents\\\\Washington\\\\EasternCascades\\\\Python\\\\CER_Timeseries_Analysis\\\\Pickle\\\\CNF23.pkl\", \"wb\"))\n",
    "# pickle.dump(CNG23, open(\"C:\\\\Users\\\\Lumbr\\\\OneDrive - UW\\\\Documents\\\\Washington\\\\EasternCascades\\\\Python\\\\CER_Timeseries_Analysis\\\\Pickle\\\\CNG23.pkl\", \"wb\"))\n",
    "# pickle.dump(CSF23, open(\"C:\\\\Users\\\\Lumbr\\\\OneDrive - UW\\\\Documents\\\\Washington\\\\EasternCascades\\\\Python\\\\CER_Timeseries_Analysis\\\\Pickle\\\\CSF23.pkl\", \"wb\"))\n",
    "# pickle.dump(CSG23, open(\"C:\\\\Users\\\\Lumbr\\\\OneDrive - UW\\\\Documents\\\\Washington\\\\EasternCascades\\\\Python\\\\CER_Timeseries_Analysis\\\\Pickle\\\\CSG23.pkl\", \"wb\"))\n",
    "\n",
    "# # the other post treatment sites \n",
    "# pickle.dump(CN20, open(\"C:\\\\Users\\\\Lumbr\\\\OneDrive - UW\\\\Documents\\\\Washington\\\\EasternCascades\\\\Python\\\\CER_Timeseries_Analysis\\\\Pickle\\\\CN20.pkl\", \"wb\"))\n",
    "# pickle.dump(CN50, open(\"C:\\\\Users\\\\Lumbr\\\\OneDrive - UW\\\\Documents\\\\Washington\\\\EasternCascades\\\\Python\\\\CER_Timeseries_Analysis\\\\Pickle\\\\CN50.pkl\", \"wb\"))\n",
    "# pickle.dump(CN60, open(\"C:\\\\Users\\\\Lumbr\\\\OneDrive - UW\\\\Documents\\\\Washington\\\\EasternCascades\\\\Python\\\\CER_Timeseries_Analysis\\\\Pickle\\\\CN60.pkl\", \"wb\"))\n",
    "# pickle.dump(CN70, open(\"C:\\\\Users\\\\Lumbr\\\\OneDrive - UW\\\\Documents\\\\Washington\\\\EasternCascades\\\\Python\\\\CER_Timeseries_Analysis\\\\Pickle\\\\CN70.pkl\", \"wb\"))\n",
    "\n",
    "# pickle.dump(CS20, open(\"C:\\\\Users\\\\Lumbr\\\\OneDrive - UW\\\\Documents\\\\Washington\\\\EasternCascades\\\\Python\\\\CER_Timeseries_Analysis\\\\Pickle\\\\CS20.pkl\", \"wb\"))\n",
    "# pickle.dump(CS50, open(\"C:\\\\Users\\\\Lumbr\\\\OneDrive - UW\\\\Documents\\\\Washington\\\\EasternCascades\\\\Python\\\\CER_Timeseries_Analysis\\\\Pickle\\\\CS50.pkl\", \"wb\"))\n",
    "# pickle.dump(CS60, open(\"C:\\\\Users\\\\Lumbr\\\\OneDrive - UW\\\\Documents\\\\Washington\\\\EasternCascades\\\\Python\\\\CER_Timeseries_Analysis\\\\Pickle\\\\CS60.pkl\", \"wb\"))\n",
    "# pickle.dump(CS120, open(\"C:\\\\Users\\\\Lumbr\\\\OneDrive - UW\\\\Documents\\\\Washington\\\\EasternCascades\\\\Python\\\\CER_Timeseries_Analysis\\\\Pickle\\\\CS120.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a181fc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now the SDD files too\n",
    "# pickle.dump(SDD_CN23, open(\"C:\\\\Users\\\\Lumbr\\\\OneDrive - UW\\\\Documents\\\\Washington\\\\EasternCascades\\\\Python\\\\CER_Timeseries_Analysis\\\\Pickle\\\\SDD_CN23.pkl\", \"wb\"))\n",
    "# pickle.dump(SDD_CS23, open(\"C:\\\\Users\\\\Lumbr\\\\OneDrive - UW\\\\Documents\\\\Washington\\\\EasternCascades\\\\Python\\\\CER_Timeseries_Analysis\\\\Pickle\\\\SDD_CS23.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa96500d",
   "metadata": {},
   "source": [
    "If we want, I can add CSVs to this instead, but let's start here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
